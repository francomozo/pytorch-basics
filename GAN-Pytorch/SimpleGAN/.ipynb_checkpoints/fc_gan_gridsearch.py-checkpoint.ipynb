{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c64d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Grid Search\n",
    "hidden_layers = [64, 128, 512]\n",
    "relu_slopes = [0.1, 0.01]\n",
    "z_dims = [64, 256, 512]\n",
    "learning_rates = [3e-4, 1e-4, 3e-3]\n",
    "\n",
    "hparams = [(hlayers, relu_slope, z_dim, lr) for hlayers in hidden_layers\n",
    "                                                for relu_slope in relu_slopes\n",
    "                                                for z_dim in z_dims\n",
    "                                                for lr in learning_rates]\n",
    "\n",
    "for (hlayers, relu_slope, z_dim, lr) in hparams:\n",
    "    \n",
    "    # architectures\n",
    "    class Discriminator(nn.Module):\n",
    "        # It is the inspector, or the policeman\n",
    "        def __init__(self, in_features):\n",
    "            super().__init__()\n",
    "\n",
    "            self.disc = nn.Sequential(\n",
    "                nn.Linear(in_features, hlayers),\n",
    "                nn.LeakyReLU(relu_slope),\n",
    "                nn.Linear(hlayers, 1),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.disc(x)\n",
    "\n",
    "\n",
    "    class Generator(nn.Module):\n",
    "        # It is the forger\n",
    "        # z_dim : dimension of noise\n",
    "        def __init__(self, z_dim, img_dim):\n",
    "            super().__init__()\n",
    "\n",
    "            self.gen = nn.Sequential(\n",
    "                nn.Linear(z_dim, hlayers),\n",
    "                nn.LeakyReLU(relu_slope),\n",
    "                nn.Linear(hlayers, img_dim),\n",
    "                nn.Tanh(),  # normalize inputs to [-1, 1] so make outputs [-1, 1]\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.gen(x)\n",
    "\n",
    "    # Hyperparameters\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f'Using {device}')\n",
    "    #lr = 3e-4 # Andrej Karpathy's tweet\n",
    "    #z_dim = 64 # can try 128, 256, etc\n",
    "    image_dim = 28 * 28 * 1  # 784\n",
    "    batch_size = 128 #32\n",
    "    num_epochs = 10\n",
    "\n",
    "    # Initializations\n",
    "    disc = Discriminator(image_dim).to(device)\n",
    "    gen = Generator(z_dim, image_dim).to(device)\n",
    "    fixed_noise = torch.randn((batch_size, z_dim)).to(device) # to see how it changes through time\n",
    "\n",
    "\n",
    "    transforms = transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    dataset_path = '/'.join(os.getcwd().split('/')[:-1]) + '/dataset/' # uglycode\n",
    "    dataset = datasets.MNIST(root=dataset_path, transform=transforms, download=False)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss() # implements the form of the GANs loss. The game is to minimize this loss\n",
    "                             # for both networks. Even though the theoretical losses are to be maximized\n",
    "                             # nn.BCELoss has a minus \"-\" at the beggining, so it is the same as minimizing\n",
    "                             # -\"the loss functions\"\n",
    "\n",
    "    writer_fake = SummaryWriter(f\"runs/hlayers_{hlayers}-relu_slope_{relu_slope}-z_dim_{z_dim}-lr_{lr}/fake\")\n",
    "    writer_real = SummaryWriter(f\"runs/hlayers_{hlayers}-relu_slope_{relu_slope}-z_dim_{z_dim}-lr_{lr}/real\")\n",
    "    writer = SummaryWriter(f\"runs/hparams/lr {lr} bsize {bsize} noise_dim {noise_dim} ft {ft}\")\n",
    "    step = 0\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (real, _) in enumerate(loader):\n",
    "            real = real.view(-1, 784).to(device)        \n",
    "\n",
    "            ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "            noise = torch.randn(batch_size, z_dim).to(device) # Gaussian distribution\n",
    "            fake = gen(noise) # generate fake images\n",
    "\n",
    "            # Part of the loss function for real images\n",
    "            disc_real = disc(real).view(-1) # output of the Discriminator on real image\n",
    "            lossD_real = criterion(disc_real, torch.ones_like(disc_real)) # torch.ones is the y_n in the\n",
    "                                                                          # doc of BCELoss *****\n",
    "            # Same but with fake images output from the generator\n",
    "            disc_fake = disc(fake).view(-1)\n",
    "            lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake)) # *****\n",
    "            # ****: for lossD_real we want the first term (y_n * log(x_n)), whether\n",
    "            #       in lossD_fake we want the second ((1 - y_n * log(1 - x_n))), so this explains\n",
    "            #       the why in torch.ones_like and torch.zeros_like\n",
    "            lossD = (lossD_real + lossD_fake) / 2\n",
    "            disc.zero_grad()\n",
    "            lossD.backward(retain_graph=True) # retain_graph=True because I want to reuse fake=gen(noise)\n",
    "                                              # in the next section\n",
    "            opt_disc.step()\n",
    "\n",
    "            ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "            # where the second option of maximizing doesn't suffer from\n",
    "            # saturating gradients\n",
    "            output = disc(fake).view(-1)\n",
    "            lossG = criterion(output, torch.ones_like(output))\n",
    "            gen.zero_grad()\n",
    "            lossG.backward()\n",
    "            opt_gen.step()\n",
    "\n",
    "            # Tensorboard code\n",
    "            if batch_idx == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                          Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "                )\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                    data = real.reshape(-1, 1, 28, 28)\n",
    "                    img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                    img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                    writer_fake.add_image(\n",
    "                        \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                    )\n",
    "                    writer_real.add_image(\n",
    "                        \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                    )\n",
    "                    step += 1\n",
    "                    \n",
    "    writer.add_hparams(\n",
    "        {\"lr\": lr, \"bsize\": bsize, \"noise_dim\": noise_dim, \"ft\": ft},\n",
    "        {\n",
    "            \"loss_D\": sum(losses_epoch_disc) / len(losses_epoch_disc),\n",
    "            \"loss_G\": sum(losses_epoch_gen) / len(losses_epoch_gen),\n",
    "        },\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
