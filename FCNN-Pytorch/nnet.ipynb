{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd020d1e955e56f60934500e026a2c8995494ca6ff329c3eac4b4ea183ec5feda91",
   "display_name": "Python 3.7.9 64-bit ('cs231n': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imports\n",
    "import torch\n",
    "import torch.nn as nn   # All the neural network modules, ex nn.linear, loss functions\n",
    "import torch.optim as optim     # SGD, Adam, etc\n",
    "import torch.nn.functional as F     # All the functions that do not have any parameters, ie: relu, tanh\n",
    "                                    # These are also include in the nn package which is confusing\n",
    "from torch.utils.data import DataLoader \n",
    "import torchvision.datasets as datasets # Standar datasets that are easy to import\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchsummary import summary # Print summary (layers and trainable params) of model\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Create fully connected network\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 3) Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# 4) Hyperparameters\n",
    "input_size = 784 # Images of size 28x28 with one channel\n",
    "hidden_size = 50 \n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Load data\n",
    "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) # From one epoch to the next, \n",
    "                                                                                      # shuffles the batches \n",
    "                                                                                      # (diffeentent images in a \n",
    "                                                                                      # batch in diff epochs)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Initialize network\n",
    "model = NN(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes).to(device)\n",
    "\n",
    "# 8) Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/3\n",
      "Epoch: 2/3\n",
      "Epoch: 3/3\n"
     ]
    }
   ],
   "source": [
    "# 9) Train network\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print(f'Epoch: {epoch + 1}/{num_epochs}')\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # Reshape to correct shape\n",
    "        data = data.reshape(data.shape[0], -1) # here i can use data.flatten(start_dim=1) to reshape\n",
    "        \n",
    "        # Forward pass\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad() # Set gradient to zero on each batch\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient descent or Adam step\n",
    "        optimizer.step()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 10) Check accuracy on training & test to see how good our model is\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking accuracy on training data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on test data\")\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Impact how the calculations are done\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "            scores = model(x) # output=64x10 (num_images_in_each_batch x predictions)\n",
    "            _, predictions = scores.max(1) # predictions.shape() = 64x1\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0) # 64\n",
    "\n",
    "        print(f'Got {num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}%') # Dos cifras significativas\n",
    "\n",
    "    model.train() # Return model to train mode, not necessary in this case since we are not training anymore\n",
    "\n",
    "# check_accuracy(train_loader, model)\n"
   ]
  },
  {
   "source": [
    "# Some exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### DATALOADER: how to extract information about the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset MNIST\n    Number of datapoints: 60000\n    Root location: dataset/\n    Split: Train\n    StandardTransform\nTransform: ToTensor() \n\n64 \n\n938 = num_samples/batch_size \n\n['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\ndataset/\n60000 \n\n<class 'torch.Tensor'>\ntorch.Size([64, 1, 28, 28])\ntorch.Size([64])\nTo flatten the dim i can use pyTorch:  torch.Size([64, 784])\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.dataset, '\\n')\n",
    "print(train_loader.batch_size , '\\n')\n",
    "print(len(train_loader), '= num_samples/batch_size', '\\n')\n",
    "\n",
    "print(train_loader.dataset.classes)\n",
    "print(train_loader.dataset.root)\n",
    "print(len(train_loader.dataset), '\\n')\n",
    "\n",
    "# Get shape of data from the dataloader\n",
    "images, labels = iter(train_loader).next()\n",
    "print(type(images))\n",
    "print(images.size()) # size and shape are correct\n",
    "print(labels.shape)\n",
    "print(\"To flatten the dim i can use pyTorch: \", images.flatten(start_dim=1).size())\n"
   ]
  },
  {
   "source": [
    "OBS: .shape is an alias for .size(), and was added to more closely match numpy. Both are correct."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### MODEL:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "From pytorch doc: \n You just have to define the forward function, and the backward function (where gradients are computed) is automatically defined for you using autograd. You can use any of the Tensor operations in the forward function. \n\nNN(\n  (fc1): Linear(in_features=784, out_features=50, bias=True)\n  (fc2): Linear(in_features=50, out_features=10, bias=True)\n) \n\n4\n<class 'torch.Tensor'> torch.Size([50, 784])\n<class 'torch.Tensor'> torch.Size([50])\n<class 'torch.Tensor'> torch.Size([10, 50])\n<class 'torch.Tensor'> torch.Size([10])\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Linear-1               [-1, 64, 50]          39,250\n            Linear-2               [-1, 64, 10]             510\n================================================================\nTotal params: 39,760\nTrainable params: 39,760\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.19\nForward/backward pass size (MB): 0.03\nParams size (MB): 0.15\nEstimated Total Size (MB): 0.37\n----------------------------------------------------------------\nNone\n\nCheck where the model is:  True\ntorch.Size([1, 784])\nOutput of the network:  tensor([[-0.1406,  0.0306,  0.4236, -0.0929, -0.0139,  0.1235, -0.1467,  0.2114,\n          0.1012,  0.0957]], device='cuda:0', grad_fn=<AddmmBackward>)\ntensor([[-0.1406,  0.0306,  0.4236, -0.0929, -0.0139,  0.1235, -0.1467,  0.2114,\n          0.1012,  0.0957]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"From pytorch doc: \\n You just have to define the forward function, and the backward function (where gradients are computed) is automatically defined for you using autograd. You can use any of the Tensor operations in the forward function. \\n\")\n",
    "\n",
    "\n",
    "####################### Model achitecture and parameters ####################\n",
    "print(model, \"\\n\")\n",
    "\n",
    "# model.parameters: Returns an iterator over module parameters.\n",
    "#                   This is typically passed to an optimizer.\n",
    "print(len(list(model.parameters())))\n",
    "for param in model.parameters():\n",
    "    print(type(param.data), param.size())\n",
    "print()\n",
    "print(summary(model, (64, 784)))\n",
    "print()\n",
    "print(\"Check where the model is: \", next(model.parameters()).is_cuda)\n",
    "\n",
    "####################### Data shape, etc ####################\n",
    "# One sample\n",
    "input = torch.randn(1, 1, 28, 28).flatten(start_dim=1).to(device=device)\n",
    "print(input.size())\n",
    "\n",
    "out = model(input)\n",
    "print(\"Output of the network: \", out)\n",
    "print(out.clone().data)\n",
    "\n"
   ]
  },
  {
   "source": [
    "OBS: .data should be used carefully, as it detaches the tensor from the computation graph and might lead to wrong results. (from https://discuss.pytorch.org/t/getting-data-from-tensor/2785/5). I can clone and then .data to get data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### TUTO: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## How to save and load pytorch models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NN(\n",
       "  (fc1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# Steps:\n",
    "#   Define and intialize the model\n",
    "#   Initialize the optimizer (ie: SGD, Adam, etc)\n",
    "#   Save general checkpoint\n",
    "#   Load the general checkpoint\n",
    "\n",
    "PATH = \"model.pt\"\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, PATH)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "# - or -\n",
    "# model.train()\n"
   ]
  },
  {
   "source": [
    "## CHECK memory usage of gpu and why is not training with cuda"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce GTX 1050 Ti\n",
      "4\n",
      "Memory Usage:\n",
      "\t Allocated: 512 bytes\n",
      "\t Cached:    2097152 bytes\n"
     ]
    }
   ],
   "source": [
    "def tensor_mem_usage(a):\n",
    "    \"\"\" Returns memory usage of tensor 'a' in bytes\n",
    "    \"\"\"\n",
    "    return a.element_size() * a.nelement()\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    vector = torch.FloatTensor(1).to('cuda')\n",
    "    print(tensor_mem_usage(vector))\n",
    "    print('Memory Usage:')\n",
    "    print('\\t Allocated:', round(\n",
    "        torch.cuda.memory_allocated(0), 1), 'bytes')\n",
    "    print('\\t Cached:   ', round(\n",
    "        torch.cuda.memory_reserved(0), 1), 'bytes')"
   ]
  },
  {
   "source": [
    "## The idea is to create a modular training loop with functions to deploy to ClusterUY"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking accuracy on training data\n",
      "Got 58887/60000 with accuracy 98.15%\n",
      "Epoch: 15.\n",
      "Epoch: 16.\n",
      "Epoch: 17.\n",
      "Epoch: 18.\n",
      "Epoch: 19.\n",
      "Epoch: 20.\n",
      "Checking accuracy on training data\n",
      "Got 59238/60000 with accuracy 98.73%\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"265.69625pt\" version=\"1.1\" viewBox=\"0 0 395.64375 265.69625\" width=\"395.64375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 265.69625 \nL 395.64375 265.69625 \nL 395.64375 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 228.14 \nL 384.94375 228.14 \nL 384.94375 10.7 \nL 50.14375 10.7 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m73c5e087af\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#m73c5e087af\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(57.410369 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"103.407386\" xlink:href=\"#m73c5e087af\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2.5 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(95.455824 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"141.452841\" xlink:href=\"#m73c5e087af\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 5.0 -->\n      <g transform=\"translate(133.501278 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.498295\" xlink:href=\"#m73c5e087af\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 7.5 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(171.546733 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"217.54375\" xlink:href=\"#m73c5e087af\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 10.0 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(206.410938 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"255.589205\" xlink:href=\"#m73c5e087af\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 12.5 -->\n      <g transform=\"translate(244.456392 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"293.634659\" xlink:href=\"#m73c5e087af\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 15.0 -->\n      <g transform=\"translate(282.501847 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"331.680114\" xlink:href=\"#m73c5e087af\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 17.5 -->\n      <g transform=\"translate(320.547301 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.725568\" xlink:href=\"#m73c5e087af\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 20.0 -->\n      <g transform=\"translate(358.592756 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- EPOCH -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      <path d=\"M 39.40625 66.21875 \nQ 28.65625 66.21875 22.328125 58.203125 \nQ 16.015625 50.203125 16.015625 36.375 \nQ 16.015625 22.609375 22.328125 14.59375 \nQ 28.65625 6.59375 39.40625 6.59375 \nQ 50.140625 6.59375 56.421875 14.59375 \nQ 62.703125 22.609375 62.703125 36.375 \nQ 62.703125 50.203125 56.421875 58.203125 \nQ 50.140625 66.21875 39.40625 66.21875 \nz\nM 39.40625 74.21875 \nQ 54.734375 74.21875 63.90625 63.9375 \nQ 73.09375 53.65625 73.09375 36.375 \nQ 73.09375 19.140625 63.90625 8.859375 \nQ 54.734375 -1.421875 39.40625 -1.421875 \nQ 24.03125 -1.421875 14.8125 8.828125 \nQ 5.609375 19.09375 5.609375 36.375 \nQ 5.609375 53.65625 14.8125 63.9375 \nQ 24.03125 74.21875 39.40625 74.21875 \nz\n\" id=\"DejaVuSans-79\"/>\n      <path d=\"M 64.40625 67.28125 \nL 64.40625 56.890625 \nQ 59.421875 61.53125 53.78125 63.8125 \nQ 48.140625 66.109375 41.796875 66.109375 \nQ 29.296875 66.109375 22.65625 58.46875 \nQ 16.015625 50.828125 16.015625 36.375 \nQ 16.015625 21.96875 22.65625 14.328125 \nQ 29.296875 6.6875 41.796875 6.6875 \nQ 48.140625 6.6875 53.78125 8.984375 \nQ 59.421875 11.28125 64.40625 15.921875 \nL 64.40625 5.609375 \nQ 59.234375 2.09375 53.4375 0.328125 \nQ 47.65625 -1.421875 41.21875 -1.421875 \nQ 24.65625 -1.421875 15.125 8.703125 \nQ 5.609375 18.84375 5.609375 36.375 \nQ 5.609375 53.953125 15.125 64.078125 \nQ 24.65625 74.21875 41.21875 74.21875 \nQ 47.75 74.21875 53.53125 72.484375 \nQ 59.328125 70.75 64.40625 67.28125 \nz\n\" id=\"DejaVuSans-67\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 43.015625 \nL 55.515625 43.015625 \nL 55.515625 72.90625 \nL 65.375 72.90625 \nL 65.375 0 \nL 55.515625 0 \nL 55.515625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-72\"/>\n     </defs>\n     <g transform=\"translate(200.182031 256.416562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"123.486328\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"202.197266\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"272.021484\" xlink:href=\"#DejaVuSans-72\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m9d3848fff2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m9d3848fff2\" y=\"210.120096\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.05 -->\n      <g transform=\"translate(20.878125 213.919315)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m9d3848fff2\" y=\"162.97697\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.10 -->\n      <g transform=\"translate(20.878125 166.776188)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m9d3848fff2\" y=\"115.833843\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.15 -->\n      <g transform=\"translate(20.878125 119.633062)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m9d3848fff2\" y=\"68.690716\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.20 -->\n      <g transform=\"translate(20.878125 72.489935)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m9d3848fff2\" y=\"21.547589\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.25 -->\n      <g transform=\"translate(20.878125 25.346808)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- LOSS HISTORY -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n      <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-73\"/>\n      <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n      <path d=\"M 44.390625 34.1875 \nQ 47.5625 33.109375 50.5625 29.59375 \nQ 53.5625 26.078125 56.59375 19.921875 \nL 66.609375 0 \nL 56 0 \nL 46.6875 18.703125 \nQ 43.0625 26.03125 39.671875 28.421875 \nQ 36.28125 30.8125 30.421875 30.8125 \nL 19.671875 30.8125 \nL 19.671875 0 \nL 9.8125 0 \nL 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.578125 72.90625 50.734375 67.671875 \nQ 56.890625 62.453125 56.890625 51.90625 \nQ 56.890625 45.015625 53.6875 40.46875 \nQ 50.484375 35.9375 44.390625 34.1875 \nz\nM 19.671875 64.796875 \nL 19.671875 38.921875 \nL 32.078125 38.921875 \nQ 39.203125 38.921875 42.84375 42.21875 \nQ 46.484375 45.515625 46.484375 51.90625 \nQ 46.484375 58.296875 42.84375 61.546875 \nQ 39.203125 64.796875 32.078125 64.796875 \nz\n\" id=\"DejaVuSans-82\"/>\n      <path d=\"M -0.203125 72.90625 \nL 10.40625 72.90625 \nL 30.609375 42.921875 \nL 50.6875 72.90625 \nL 61.28125 72.90625 \nL 35.5 34.71875 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 34.71875 \nz\n\" id=\"DejaVuSans-89\"/>\n     </defs>\n     <g transform=\"translate(14.798438 155.999687)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"55.666016\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"134.376953\" xlink:href=\"#DejaVuSans-83\"/>\n      <use x=\"197.853516\" xlink:href=\"#DejaVuSans-83\"/>\n      <use x=\"261.330078\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"293.117188\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"368.3125\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"397.804688\" xlink:href=\"#DejaVuSans-83\"/>\n      <use x=\"461.28125\" xlink:href=\"#DejaVuSans-84\"/>\n      <use x=\"522.365234\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"601.076172\" xlink:href=\"#DejaVuSans-82\"/>\n      <use x=\"670.464844\" xlink:href=\"#DejaVuSans-89\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pe214ccb9d2)\" d=\"M 65.361932 20.583636 \nL 80.580114 64.167833 \nL 95.798295 66.046734 \nL 111.016477 167.538668 \nL 126.234659 124.328356 \nL 141.452841 198.461756 \nL 156.671023 82.143571 \nL 171.889205 90.368039 \nL 187.107386 129.205389 \nL 202.325568 176.833201 \nL 217.54375 131.82207 \nL 232.761932 196.581777 \nL 247.980114 218.256364 \nL 263.198295 85.77869 \nL 278.416477 203.096613 \nL 293.634659 196.39186 \nL 308.852841 204.106214 \nL 324.071023 180.745135 \nL 339.289205 186.764074 \nL 354.507386 192.481253 \nL 369.725568 204.002091 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 228.14 \nL 50.14375 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 228.14 \nL 384.94375 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 228.14 \nL 384.94375 228.14 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 10.7 \nL 384.94375 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe214ccb9d2\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"10.7\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# PyTorch: Remember to first initialize the model and optimizer, then load the dictionary locally.\n",
    "model = NN(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Load model\n",
    "PATH = \"checkpoints/model_epoch15.pt\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "EPOCH = checkpoint['epoch']\n",
    "LOSS_HISTORY = checkpoint['loss_history']\n",
    "\n",
    "# Set model to train mode\n",
    "model.train()\n",
    "\n",
    "# Check accuracy on training data with loaded model \n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "# Train a little bit more\n",
    "for epoch in range(EPOCH, 21):\n",
    "    print(f'Epoch: {epoch}.')\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # Reshape to correct shape\n",
    "        data = data.reshape(data.shape[0], -1) # here i can use data.flatten(start_dim=1) to reshape\n",
    "        \n",
    "        # Forward pass\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad() # Set gradient to zero on each batch\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient descent or Adam step\n",
    "        optimizer.step()    \n",
    "\n",
    "    LOSS_HISTORY.append(loss.clone().detach().cpu().numpy())\n",
    "\n",
    "# Check accuracy on training data with model more trained  \n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "# Print loss history\n",
    "plt.plot(LOSS_HISTORY)\n",
    "plt.ylabel('LOSS HISTORY')\n",
    "plt.xlabel('EPOCH')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## GPU memory utilization estimation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pre-training Memory Usage:\n",
      "\t Allocated: 0.0000000000 MB.\n",
      "\t Cached: 0.0000000000 MB.\n",
      "After Memory Usage:\n",
      "\t Allocated: 0.8061523438 MB. Difference: 0.80615234375 MB.\n",
      "\t Cached: 2.0000000000 MB. Difference: 2.0 MB.\n"
     ]
    }
   ],
   "source": [
    "# total consumed GPU memory = GPU memory for parameters x 2 (one for value, one for gradient) + GPU memory for storing forward and backward responses + inputs\n",
    "# https://discuss.pytorch.org/t/gpu-memory-estimation-given-a-network/1713\n",
    "\n",
    "# Mi estimation of memory is (using summary(model, (64,784))):\n",
    "#   39760 params = 0.15 MB, ok\n",
    "#   input size = 0.19 MB, ok\n",
    "#   forward/backward pass size = 0.03 MB\n",
    "#   (according to summary) estimated total size = 0.37 MB\n",
    "\n",
    "# OBS: th estiamted total size if for params, input size, and forward pass. Not the loss\n",
    "# neither the back pass. Total is 0.80 MB, so the back pass is the same as estimated total size\n",
    "\n",
    "# OBS2: Each batch of images is 200704 bytes, so each entry is 4 bytes (64*28*28 = 200704bytes)\n",
    "\n",
    "\n",
    "# Now i will check if that is accurate. Have to estimate the memory for initialization of CUDA\n",
    "before_allocated = torch.cuda.memory_allocated(0)/1024**2\n",
    "befores_cached = torch.cuda.memory_reserved(0)/1024**2\n",
    "\n",
    "# The allocated memory is the memory that is currently used to store Tensors on the GPU.\n",
    "# The cached memory is the memory that is currently used on the GPU by pytorch (as can be seen in nvidia-smi).\n",
    "\n",
    "print('Pre-training Memory Usage:')\n",
    "print(f'\\t Allocated: {before_allocated:.10f} MB.')\n",
    "print(f'\\t Cached: {befores_cached:.10f} MB.')\n",
    "\n",
    "input_size = 784\n",
    "hidden_size = 50 \n",
    "num_classes = 10\n",
    "learning_rate=0.001\n",
    "batch_size = 64\n",
    "\n",
    "# Forward + backward pass\n",
    "model = NN(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "images, labels = iter(train_loader).next()\n",
    "images = images.to(device)\n",
    "\n",
    "labels = labels.to(device)\n",
    "\n",
    "images = images.flatten(start_dim=1)\n",
    "\n",
    "scores = model(images)\n",
    "loss = criterion(scores, labels)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()    \n",
    "\n",
    "\n",
    "after_allocated = torch.cuda.memory_allocated(0)/1024**2\n",
    "after_cached = torch.cuda.memory_reserved(0)/1024**2\n",
    "print('After Memory Usage:')\n",
    "print(f'\\t Allocated: {after_allocated:.10f} MB. Difference: {(after_allocated - before_allocated)} MB.')\n",
    "print(f'\\t Cached: {after_cached:.10f} MB. Difference: {(after_cached - befores_cached)} MB.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pre-training Memory Usage:\n",
      "\t Allocated: 0.0000000000 MB.\n",
      "\t Cached: 0.0000000000 MB.\n",
      "After Memory Usage:\n",
      "\t Allocated: 0.0004882812 MB. Difference: 0.00048828125 MB.\n",
      "\t Cached: 2.0000000000 MB. Difference: 2.0 MB.\n"
     ]
    }
   ],
   "source": [
    "# CUDA INITIALIZATION MEMORY\n",
    "\n",
    "before_allocated = torch.cuda.memory_allocated(0)/1024**2\n",
    "befores_cached = torch.cuda.memory_reserved(0)/1024**2\n",
    "\n",
    "print('Pre-training Memory Usage:')\n",
    "print(f'\\t Allocated: {before_allocated:.10f} MB.')\n",
    "print(f'\\t Cached: {befores_cached:.10f} MB.')\n",
    "\n",
    "a=torch.cuda.FloatTensor(1)\n",
    "\n",
    "after_allocated = torch.cuda.memory_allocated(0)/1024**2\n",
    "after_cached = torch.cuda.memory_reserved(0)/1024**2\n",
    "print('After Memory Usage:')\n",
    "print(f'\\t Allocated: {after_allocated:.10f} MB. Difference: {(after_allocated - before_allocated)} MB.')\n",
    "print(f'\\t Cached: {after_cached:.10f} MB. Difference: {(after_cached - befores_cached)} MB.')\n",
    "\n",
    "# 0.00048828125 MB"
   ]
  },
  {
   "source": [
    "OBS: pytorch estimates one pass of the newtork in 0.80MB, but with nvidia-smi i get from 400MB to 800MB aprox, i have to check this. \n",
    "Just initializing cuda, with nvidia-smi I get around 456 MB of difference\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### CUDA memory management: https://pytorch.org/docs/master/notes/cuda.html#cuda-memory-management"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}